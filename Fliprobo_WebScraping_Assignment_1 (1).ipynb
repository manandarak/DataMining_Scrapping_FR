{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d6543a-c48f-49bb-a48c-f98cfcd43d52",
   "metadata": {},
   "source": [
    "## QUESTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e54af2d7-7936-4981-9ddd-fe298b4a54e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Header                            Text\n",
      "0     h1  WikipediaThe Free Encyclopedia\n",
      "1     h2              1,000,000+articles\n",
      "2     h2                100,000+articles\n",
      "3     h2                 10,000+articles\n",
      "4     h2                  1,000+articles\n",
      "5     h2                    100+articles\n",
      "6     h3             ウィキペディアは売り物ではありません。\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h1</td>\n",
       "      <td>WikipediaThe Free Encyclopedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h2</td>\n",
       "      <td>1,000,000+articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h2</td>\n",
       "      <td>100,000+articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h2</td>\n",
       "      <td>10,000+articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h2</td>\n",
       "      <td>1,000+articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>h2</td>\n",
       "      <td>100+articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>h3</td>\n",
       "      <td>ウィキペディアは売り物ではありません。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Header                            Text\n",
       "0     h1  WikipediaThe Free Encyclopedia\n",
       "1     h2              1,000,000+articles\n",
       "2     h2                100,000+articles\n",
       "3     h2                 10,000+articles\n",
       "4     h2                  1,000+articles\n",
       "5     h2                    100+articles\n",
       "6     h3             ウィキペディアは売り物ではありません。"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def scrape(link, class_):\n",
    "    headers = []\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    header_tags = soup.find_all(class_=class_)\n",
    "\n",
    "    for i in range(1, 7):\n",
    "        header_tag = f'h{i}'\n",
    "        for header in soup.find_all(header_tag):\n",
    "            headers.append({'Header': header_tag, 'Text': header.get_text(strip=True)})\n",
    "    \n",
    "    df = pd.DataFrame(headers)\n",
    "    return df\n",
    "\n",
    "df = scrape('https://wikipedia.org', 'restnt-info cursor')\n",
    "print(df) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a18b5-49be-458d-aa2e-ac41f0801408",
   "metadata": {},
   "source": [
    "## QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2e0884a-43a9-4fcc-a88f-226940c3dc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title    Year          Rating\n",
      "0                                         IMDb Charts    1994  9.3 (2.9M)Rate\n",
      "1                         1. The Shawshank Redemption  2h 22m    9.2 (2M)Rate\n",
      "2                                    2. The Godfather       A  9.0 (2.9M)Rate\n",
      "3                                  3. The Dark Knight    1972  9.0 (1.4M)Rate\n",
      "4                           4. The Godfather: Part II  2h 55m  9.0 (872K)Rate\n",
      "5                                     5. 12 Angry Men       A  9.0 (1.5M)Rate\n",
      "6                                 6. Schindler's List    2008    9.0 (2M)Rate\n",
      "7    7. The Lord of the Rings: The Return of the King  2h 32m  8.9 (2.2M)Rate\n",
      "8                                     8. Pulp Fiction      UA    8.9 (2M)Rate\n",
      "9   9. The Lord of the Rings: The Fellowship of th...    1974  8.8 (816K)Rate\n",
      "10                10. Il Buono, Il Brutto, Il Cattivo  3h 22m  8.8 (2.3M)Rate\n",
      "11                                   11. Forrest Gump       A  8.8 (1.8M)Rate\n",
      "12          12. The Lord of the Rings: The Two Towers    1957  8.8 (2.3M)Rate\n",
      "13                                     13. Fight Club  1h 36m  8.8 (2.6M)Rate\n",
      "14                                      14. Inception       U  8.7 (1.4M)Rate\n",
      "15  15. Star Wars: Episode V - The Empire Strikes ...    1993  8.7 (2.1M)Rate\n",
      "16                                     16. The Matrix  3h 15m  8.7 (1.3M)Rate\n",
      "17                                     17. GoodFellas       A  8.7 (1.1M)Rate\n",
      "18                18. One Flew Over the Cuckoo's Nest    2003  8.6 (1.8M)Rate\n",
      "19                                          19. Se7en  3h 21m  8.7 (2.1M)Rate\n",
      "20                                   20. Interstellar       U  8.6 (502K)Rate\n",
      "21                          21. It's a Wonderful Life    1994  8.6 (369K)Rate\n",
      "22                           22. Shichinin No Samurai  2h 34m  8.6 (1.6M)Rate\n",
      "23                       23. The Silence of the Lambs       A  8.6 (1.5M)Rate\n",
      "24                            24. Saving Private Ryan    2001  8.6 (805K)Rate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMDb Charts</td>\n",
       "      <td>1994</td>\n",
       "      <td>9.3 (2.9M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1. The Shawshank Redemption</td>\n",
       "      <td>2h 22m</td>\n",
       "      <td>9.2 (2M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2. The Godfather</td>\n",
       "      <td>A</td>\n",
       "      <td>9.0 (2.9M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3. The Dark Knight</td>\n",
       "      <td>1972</td>\n",
       "      <td>9.0 (1.4M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4. The Godfather: Part II</td>\n",
       "      <td>2h 55m</td>\n",
       "      <td>9.0 (872K)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5. 12 Angry Men</td>\n",
       "      <td>A</td>\n",
       "      <td>9.0 (1.5M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6. Schindler's List</td>\n",
       "      <td>2008</td>\n",
       "      <td>9.0 (2M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7. The Lord of the Rings: The Return of the King</td>\n",
       "      <td>2h 32m</td>\n",
       "      <td>8.9 (2.2M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8. Pulp Fiction</td>\n",
       "      <td>UA</td>\n",
       "      <td>8.9 (2M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9. The Lord of the Rings: The Fellowship of th...</td>\n",
       "      <td>1974</td>\n",
       "      <td>8.8 (816K)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10. Il Buono, Il Brutto, Il Cattivo</td>\n",
       "      <td>3h 22m</td>\n",
       "      <td>8.8 (2.3M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11. Forrest Gump</td>\n",
       "      <td>A</td>\n",
       "      <td>8.8 (1.8M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12. The Lord of the Rings: The Two Towers</td>\n",
       "      <td>1957</td>\n",
       "      <td>8.8 (2.3M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13. Fight Club</td>\n",
       "      <td>1h 36m</td>\n",
       "      <td>8.8 (2.6M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14. Inception</td>\n",
       "      <td>U</td>\n",
       "      <td>8.7 (1.4M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15. Star Wars: Episode V - The Empire Strikes ...</td>\n",
       "      <td>1993</td>\n",
       "      <td>8.7 (2.1M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16. The Matrix</td>\n",
       "      <td>3h 15m</td>\n",
       "      <td>8.7 (1.3M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17. GoodFellas</td>\n",
       "      <td>A</td>\n",
       "      <td>8.7 (1.1M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18. One Flew Over the Cuckoo's Nest</td>\n",
       "      <td>2003</td>\n",
       "      <td>8.6 (1.8M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19. Se7en</td>\n",
       "      <td>3h 21m</td>\n",
       "      <td>8.7 (2.1M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20. Interstellar</td>\n",
       "      <td>U</td>\n",
       "      <td>8.6 (502K)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21. It's a Wonderful Life</td>\n",
       "      <td>1994</td>\n",
       "      <td>8.6 (369K)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22. Shichinin No Samurai</td>\n",
       "      <td>2h 34m</td>\n",
       "      <td>8.6 (1.6M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23. The Silence of the Lambs</td>\n",
       "      <td>A</td>\n",
       "      <td>8.6 (1.5M)Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24. Saving Private Ryan</td>\n",
       "      <td>2001</td>\n",
       "      <td>8.6 (805K)Rate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title    Year          Rating\n",
       "0                                         IMDb Charts    1994  9.3 (2.9M)Rate\n",
       "1                         1. The Shawshank Redemption  2h 22m    9.2 (2M)Rate\n",
       "2                                    2. The Godfather       A  9.0 (2.9M)Rate\n",
       "3                                  3. The Dark Knight    1972  9.0 (1.4M)Rate\n",
       "4                           4. The Godfather: Part II  2h 55m  9.0 (872K)Rate\n",
       "5                                     5. 12 Angry Men       A  9.0 (1.5M)Rate\n",
       "6                                 6. Schindler's List    2008    9.0 (2M)Rate\n",
       "7    7. The Lord of the Rings: The Return of the King  2h 32m  8.9 (2.2M)Rate\n",
       "8                                     8. Pulp Fiction      UA    8.9 (2M)Rate\n",
       "9   9. The Lord of the Rings: The Fellowship of th...    1974  8.8 (816K)Rate\n",
       "10                10. Il Buono, Il Brutto, Il Cattivo  3h 22m  8.8 (2.3M)Rate\n",
       "11                                   11. Forrest Gump       A  8.8 (1.8M)Rate\n",
       "12          12. The Lord of the Rings: The Two Towers    1957  8.8 (2.3M)Rate\n",
       "13                                     13. Fight Club  1h 36m  8.8 (2.6M)Rate\n",
       "14                                      14. Inception       U  8.7 (1.4M)Rate\n",
       "15  15. Star Wars: Episode V - The Empire Strikes ...    1993  8.7 (2.1M)Rate\n",
       "16                                     16. The Matrix  3h 15m  8.7 (1.3M)Rate\n",
       "17                                     17. GoodFellas       A  8.7 (1.1M)Rate\n",
       "18                18. One Flew Over the Cuckoo's Nest    2003  8.6 (1.8M)Rate\n",
       "19                                          19. Se7en  3h 21m  8.7 (2.1M)Rate\n",
       "20                                   20. Interstellar       U  8.6 (502K)Rate\n",
       "21                          21. It's a Wonderful Life    1994  8.6 (369K)Rate\n",
       "22                           22. Shichinin No Samurai  2h 34m  8.6 (1.6M)Rate\n",
       "23                       23. The Silence of the Lambs       A  8.6 (1.5M)Rate\n",
       "24                            24. Saving Private Ryan    2001  8.6 (805K)Rate"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define headers with a User-Agent\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "# }\n",
    "\n",
    "# # Make the request with headers\n",
    "# page = requests.get('https://www.imdb.com/chart/top/', headers=headers)\n",
    "\n",
    "# # Check if the request was successful\n",
    "# if page.status_code == 200:\n",
    "#     soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#     # Now you can parse the HTML content with BeautifulSoup\n",
    "#     # Example: print the title of the webpage\n",
    "#     print(soup.title)\n",
    "# else:\n",
    "#     print(\"Failed to retrieve the page\")\n",
    "\n",
    "\n",
    "# titles = []\n",
    "# title_elements = soup.find_all('h3', class_='ipc-title__text')\n",
    "\n",
    "# for title in title_elements[:50]:\n",
    "#     titles.append(title.text.strip())\n",
    "\n",
    "\n",
    "# years = []\n",
    "# year_elements = soup.find_all('span', class_='sc-b189961a-8 kLaxqf cli-title-metadata-item')\n",
    "\n",
    "# for year in year_elements[:50]:\n",
    "#     years.append(year.text.strip()) \n",
    "\n",
    "\n",
    "\n",
    "# ratings = soup.find_all('span', class_='sc-b189961a-1 kcfvgk')\n",
    "\n",
    "# ratings = []\n",
    "# rating_elements = soup.find_all('span', class_='sc-b189961a-1 kcfvgk')\n",
    "\n",
    "# for rating in rating_elements[:50]:\n",
    "#     ratings.append(rating.text)\n",
    "\n",
    "# min_length = min(len(titles), len(years), len(ratings))\n",
    "# titles = titles[:min_length]\n",
    "# years = years[:min_length]\n",
    "# ratings = ratings[:min_length]\n",
    "\n",
    "# # Create DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'Title': titles,\n",
    "#     'Year': years,\n",
    "#     'Rating': ratings\n",
    "# })\n",
    "\n",
    "# # Print DataFrame\n",
    "# print(df)\n",
    "# df\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_top_imdb_data(url):\n",
    "    # Define headers with a User-Agent\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                      '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Make the request with headers\n",
    "    page = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # Extract titles\n",
    "        titles = []\n",
    "        title_elements = soup.find_all('h3', class_='ipc-title__text')\n",
    "        for title in title_elements[:50]:\n",
    "            titles.append(title.text.strip())\n",
    "\n",
    "        # Extract years\n",
    "        years = []\n",
    "        year_elements = soup.find_all('span', class_='sc-b189961a-8 kLaxqf cli-title-metadata-item')\n",
    "        for year in year_elements[:50]:\n",
    "            years.append(year.text.strip())\n",
    "\n",
    "        # Extract ratings\n",
    "        ratings = []\n",
    "        rating_elements = soup.find_all('span', class_='sc-b189961a-1 kcfvgk')\n",
    "        for rating in rating_elements[:50]:\n",
    "            ratings.append(rating.text.strip())\n",
    "\n",
    "        # Ensure all lists are the same length\n",
    "        min_length = min(len(titles), len(years), len(ratings))\n",
    "        titles = titles[:min_length]\n",
    "        years = years[:min_length]\n",
    "        ratings = ratings[:min_length]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Title': titles,\n",
    "            'Year': years,\n",
    "            'Rating': ratings\n",
    "        })\n",
    "\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {page.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "url = 'https://www.imdb.com/chart/top/'\n",
    "top_imdb_df = scrape_top_imdb_data(url)\n",
    "if top_imdb_df is not None:\n",
    "    print(top_imdb_df)\n",
    "\n",
    "top_imdb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9815e94-a396-4446-b22e-3b42819d3bd0",
   "metadata": {},
   "source": [
    "## QUESTION 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21d0a971-d0ca-49de-b2a5-88b870abe973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Titles Ratings\n",
      "0   1Ramayana: The Legend of Prince Rama     9.2\n",
      "1                             212th Fail     8.9\n",
      "2                               3Nayakan     8.7\n",
      "3                              4Gol Maal     8.5\n",
      "4                            5Anbe Sivam     8.6\n",
      "5            6Rocketry: The Nambi Effect     8.7\n",
      "6                           7777 Charlie     8.7\n",
      "7                     8Pariyerum Perumal     8.7\n",
      "8                                 9#Home     8.8\n",
      "9                     10Manichitrathazhu     8.7\n",
      "10                            113 Idiots     8.4\n",
      "11                         12Apur Sansar     8.4\n",
      "12                            13Kireedam     8.9\n",
      "13                   14C/O Kancharapalem     8.8\n",
      "14                     15Soorarai Pottru     8.7\n",
      "15                   16Kumbalangi Nights     8.5\n",
      "16                        17Black Friday     8.4\n",
      "17                           18Sandesham     9.0\n",
      "18                            19Jai Bhim     8.7\n",
      "19                              20Jersey     8.5\n",
      "20                    21Taare Zameen Par     8.3\n",
      "21                     22Laapataa Ladies     8.5\n",
      "22                         23Maya Bazaar     9.1\n",
      "23                                  2496     8.5\n",
      "24                           25Natsamrat     8.8\n",
      "25                              26Dangal     8.3\n",
      "26                              27Kaithi     8.4\n",
      "27                          28Sita Ramam     8.5\n",
      "28                          29Drishyam 2     8.4\n",
      "29                          30Visaaranai     8.4\n",
      "30                        31Thevar Magan     8.7\n",
      "31                              32Asuran     8.4\n",
      "32                        33Nadodikkattu     8.8\n",
      "33                          34Thalapathi     8.5\n",
      "34                 35Sarpatta Parambarai     8.5\n",
      "35                            36Drishyam     8.3\n",
      "36                        37Thani Oruvan     8.4\n",
      "37                38Nuvvu Naaku Nachchav     8.8\n",
      "38                             39Anniyan     8.3\n",
      "39                        40Vada Chennai     8.4\n",
      "40                  41Jaane Bhi Do Yaaro     8.3\n",
      "41                   42Khosla Ka Ghosla!     8.2\n",
      "42                            43Ratsasan     8.3\n",
      "43                             44Peranbu     8.7\n",
      "44                           45Aparajito     8.2\n",
      "45                        46Sardar Udham     8.4\n",
      "46                      47Bangalore Days     8.3\n",
      "47                           48Devasuram     8.7\n",
      "48                              49Premam     8.3\n",
      "49                               50Satya     8.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212th Fail</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3Nayakan</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4Gol Maal</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5Anbe Sivam</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6Rocketry: The Nambi Effect</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7777 Charlie</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8Pariyerum Perumal</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9#Home</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10Manichitrathazhu</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>113 Idiots</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12Apur Sansar</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13Kireedam</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14C/O Kancharapalem</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15Soorarai Pottru</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16Kumbalangi Nights</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17Black Friday</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18Sandesham</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19Jai Bhim</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20Jersey</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21Taare Zameen Par</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22Laapataa Ladies</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23Maya Bazaar</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2496</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25Natsamrat</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26Dangal</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27Kaithi</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28Sita Ramam</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29Drishyam 2</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30Visaaranai</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31Thevar Magan</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32Asuran</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33Nadodikkattu</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34Thalapathi</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35Sarpatta Parambarai</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36Drishyam</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37Thani Oruvan</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38Nuvvu Naaku Nachchav</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39Anniyan</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40Vada Chennai</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41Jaane Bhi Do Yaaro</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42Khosla Ka Ghosla!</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43Ratsasan</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44Peranbu</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45Aparajito</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46Sardar Udham</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47Bangalore Days</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48Devasuram</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49Premam</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50Satya</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Titles Ratings\n",
       "0   1Ramayana: The Legend of Prince Rama     9.2\n",
       "1                             212th Fail     8.9\n",
       "2                               3Nayakan     8.7\n",
       "3                              4Gol Maal     8.5\n",
       "4                            5Anbe Sivam     8.6\n",
       "5            6Rocketry: The Nambi Effect     8.7\n",
       "6                           7777 Charlie     8.7\n",
       "7                     8Pariyerum Perumal     8.7\n",
       "8                                 9#Home     8.8\n",
       "9                     10Manichitrathazhu     8.7\n",
       "10                            113 Idiots     8.4\n",
       "11                         12Apur Sansar     8.4\n",
       "12                            13Kireedam     8.9\n",
       "13                   14C/O Kancharapalem     8.8\n",
       "14                     15Soorarai Pottru     8.7\n",
       "15                   16Kumbalangi Nights     8.5\n",
       "16                        17Black Friday     8.4\n",
       "17                           18Sandesham     9.0\n",
       "18                            19Jai Bhim     8.7\n",
       "19                              20Jersey     8.5\n",
       "20                    21Taare Zameen Par     8.3\n",
       "21                     22Laapataa Ladies     8.5\n",
       "22                         23Maya Bazaar     9.1\n",
       "23                                  2496     8.5\n",
       "24                           25Natsamrat     8.8\n",
       "25                              26Dangal     8.3\n",
       "26                              27Kaithi     8.4\n",
       "27                          28Sita Ramam     8.5\n",
       "28                          29Drishyam 2     8.4\n",
       "29                          30Visaaranai     8.4\n",
       "30                        31Thevar Magan     8.7\n",
       "31                              32Asuran     8.4\n",
       "32                        33Nadodikkattu     8.8\n",
       "33                          34Thalapathi     8.5\n",
       "34                 35Sarpatta Parambarai     8.5\n",
       "35                            36Drishyam     8.3\n",
       "36                        37Thani Oruvan     8.4\n",
       "37                38Nuvvu Naaku Nachchav     8.8\n",
       "38                             39Anniyan     8.3\n",
       "39                        40Vada Chennai     8.4\n",
       "40                  41Jaane Bhi Do Yaaro     8.3\n",
       "41                   42Khosla Ka Ghosla!     8.2\n",
       "42                            43Ratsasan     8.3\n",
       "43                             44Peranbu     8.7\n",
       "44                           45Aparajito     8.2\n",
       "45                        46Sardar Udham     8.4\n",
       "46                      47Bangalore Days     8.3\n",
       "47                           48Devasuram     8.7\n",
       "48                              49Premam     8.3\n",
       "49                               50Satya     8.3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define headers with a User-Agent\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Make the request with headers\n",
    "page = requests.get('https://www.imdb.com/india/top-rated-indian-movies/', headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if page.status_code == 200:\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Find all span elements with class 'sc-551fcf62-4 iYlNDF'\n",
    "    title_elements = soup.find_all('span', class_='sc-551fcf62-4 iYlNDF')\n",
    "\n",
    "    # Extract titles from title elements\n",
    "    titles = [title.text.strip() for title in title_elements[:50]]\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the page\")\n",
    "\n",
    "rating = soup.find_all('span', class_='ipc-rating-star ipc-rating-star--base ipc-rating-star--imdb ratingGroup--imdb-rating')\n",
    "\n",
    "    # Extract titles from title elements\n",
    "ratings = [rating.text.strip() for rating in rating[:50]]\n",
    "\n",
    "dataframe = {\n",
    "    \"Titles\" : titles,\n",
    "    \"Ratings\" : ratings,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dataframe)\n",
    "print(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783328fd-ad29-46be-942d-63eb58e2233b",
   "metadata": {},
   "source": [
    "## QUESTION 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f6e6264-4a3d-4a3c-baa3-e74f044f9aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Name                     Term\n",
      "0           Shri Ram Nath Kovind  14th President of India\n",
      "1          Shri Pranab Mukherjee  13th President of India\n",
      "2   Smt Pratibha Devisingh Patil  12th President of India\n",
      "3         DR. A.P.J. Abdul Kalam  11th President of India\n",
      "4           Shri K. R. Narayanan  10th President of India\n",
      "5        Dr Shankar Dayal Sharma  9th  President of India\n",
      "6            Shri R Venkataraman   8th President of India\n",
      "7               Giani Zail Singh   7th President of India\n",
      "8      Shri Neelam Sanjiva Reddy   6th President of India\n",
      "9       Dr. Fakhruddin Ali Ahmed   5th President of India\n",
      "10  Shri Varahagiri Venkata Giri   4th President of India\n",
      "11              Dr. Zakir Husain   3rd President of India\n",
      "12  Dr. Sarvepalli Radhakrishnan   2nd President of India\n",
      "13           Dr. Rajendra Prasad   1st President of India\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>14th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>13th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>12th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>11th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>10th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>9th  President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>8th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>7th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>6th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>5th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>4th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>3rd President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>2nd President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>1st President of India</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name                     Term\n",
       "0           Shri Ram Nath Kovind  14th President of India\n",
       "1          Shri Pranab Mukherjee  13th President of India\n",
       "2   Smt Pratibha Devisingh Patil  12th President of India\n",
       "3         DR. A.P.J. Abdul Kalam  11th President of India\n",
       "4           Shri K. R. Narayanan  10th President of India\n",
       "5        Dr Shankar Dayal Sharma  9th  President of India\n",
       "6            Shri R Venkataraman   8th President of India\n",
       "7               Giani Zail Singh   7th President of India\n",
       "8      Shri Neelam Sanjiva Reddy   6th President of India\n",
       "9       Dr. Fakhruddin Ali Ahmed   5th President of India\n",
       "10  Shri Varahagiri Venkata Giri   4th President of India\n",
       "11              Dr. Zakir Husain   3rd President of India\n",
       "12  Dr. Sarvepalli Radhakrishnan   2nd President of India\n",
       "13           Dr. Rajendra Prasad   1st President of India"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# page = requests.get ('https://www.presidentofindia.gov.in/index.php/former-presidents')\n",
    "# soup = BeautifulSoup(page.content,'html.parser')\n",
    "# header_tages =  soup.find_all('div',class_= 'desc-sec')\n",
    "\n",
    "# presidents = []\n",
    "# for president in header_tages:\n",
    "#     name = president.find('h3').text.strip()\n",
    "#     term = president.find('h5').text.strip()\n",
    "#     presidents.append({'Name': name, 'Term': term})\n",
    "\n",
    "# # Create a Pandas DataFrame from the extracted data\n",
    "# df = pd.DataFrame(presidents)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# df\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def scrap(link, class_):\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    header_tags = soup.find_all(class_=class_)\n",
    "\n",
    "    presidents = []\n",
    "    for president in header_tags:\n",
    "        name = president.find('h3').text.strip()\n",
    "        term = president.find('h5').text.strip()\n",
    "        presidents.append({'Name': name, 'Term': term})\n",
    "    \n",
    "    df = pd.DataFrame(presidents)\n",
    "    return df\n",
    "\n",
    "link = \"https://www.presidentofindia.gov.in/index.php/former-presidents\"\n",
    "class_ = \"desc-sec\"\n",
    "df = scrap(link, class_)\n",
    "print(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72beb13-08ff-481a-ab2d-523263a0db68",
   "metadata": {},
   "source": [
    "## QUESTION 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a556ed-c980-4865-8b6e-22af01f54467",
   "metadata": {},
   "source": [
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e37f3b20-3771-4716-92c6-40ea68617b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised:  Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//h3[@class=\"si-team-name\"]/span[@class=\"si-fname si-text\"]\"}\n",
      "  (Session info: chrome=126.0.6478.63); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF71E77EEA2+31554]\n",
      "\t(No symbol) [0x00007FF71E6F7ED9]\n",
      "\t(No symbol) [0x00007FF71E5B872A]\n",
      "\t(No symbol) [0x00007FF71E608434]\n",
      "\t(No symbol) [0x00007FF71E60853C]\n",
      "\t(No symbol) [0x00007FF71E5FBBAC]\n",
      "\t(No symbol) [0x00007FF71E62D06F]\n",
      "\t(No symbol) [0x00007FF71E5FBA76]\n",
      "\t(No symbol) [0x00007FF71E62D240]\n",
      "\t(No symbol) [0x00007FF71E64C977]\n",
      "\t(No symbol) [0x00007FF71E62CDD3]\n",
      "\t(No symbol) [0x00007FF71E5FA33B]\n",
      "\t(No symbol) [0x00007FF71E5FAED1]\n",
      "\tGetHandleVerifier [0x00007FF71EA88B1D+3217341]\n",
      "\tGetHandleVerifier [0x00007FF71EAD5AE3+3532675]\n",
      "\tGetHandleVerifier [0x00007FF71EACB0E0+3489152]\n",
      "\tGetHandleVerifier [0x00007FF71E82E776+750614]\n",
      "\t(No symbol) [0x00007FF71E70375F]\n",
      "\t(No symbol) [0x00007FF71E6FEB14]\n",
      "\t(No symbol) [0x00007FF71E6FECA2]\n",
      "\t(No symbol) [0x00007FF71E6EE16F]\n",
      "\tBaseThreadInitThunk [0x00007FF9C56E257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF9C5F6AF28+40]\n",
      "\n",
      "            Team Matches Points Rating\n",
      "0          INDIA      42   5117    122\n",
      "1      AUSTRALIA      34   3936    116\n",
      "2   SOUTH AFRICA      30   3357    112\n",
      "3       PAKISTAN      26   2762    106\n",
      "4    NEW ZEALAND      33   3349    101\n",
      "5        ENGLAND      28   2672     95\n",
      "6      SRI LANKA      47   4363     93\n",
      "7     BANGLADESH      40   3453     86\n",
      "8    AFGHANISTAN      31   2477     80\n",
      "9    WEST INDIES      32   2205     69\n",
      "10       IRELAND      22   1091     50\n",
      "11      ZIMBABWE      24   1181     49\n",
      "12      SCOTLAND      25   1207     48\n",
      "13   NETHERLANDS      34   1482     44\n",
      "14        CANADA       9    320     36\n",
      "15       NAMIBIA      20    711     36\n",
      "16         NEPAL      35   1095     31\n",
      "17          OMAN      21    509     24\n",
      "18           USA      20    410     21\n",
      "19           UAE      30    345     12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def scrape_icc_team_rankings(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'si-body'))\n",
    "    )\n",
    "    \n",
    "   \n",
    "    team_names = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "    rows = driver.find_elements(By.XPATH, '//div[@class=\"si-table-row\"]')\n",
    "    \n",
    "    for row in rows:\n",
    "        try:\n",
    "            team_name = row.find_element(By.XPATH, './/h3[@class=\"si-team-name\"]/span[@class=\"si-fname si-text\"]').text\n",
    "            match = row.find_element(By.XPATH, './/div[@class=\"si-table-data si-matches\"]/span[@class=\"si-text\"]').text\n",
    "            point = row.find_element(By.XPATH, './/div[@class=\"si-table-data si-pts\"]/span[@class=\"si-text\"]').text\n",
    "            rating = row.find_element(By.XPATH, './/div[@class=\"si-table-data si-rating\"]/span[@class=\"si-text\"]').text\n",
    "            \n",
    "            team_names.append(team_name)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Exception raised: \", e)\n",
    "    driver.quit()\n",
    "    data = {\n",
    "        'Team': team_names,\n",
    "        'Matches': matches,\n",
    "        'Points': points,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "url = 'https://www.icc-cricket.com/rankings/team-rankings/mens/odi'\n",
    "df = scrape_icc_team_rankings(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b236253-9b57-45c1-b4f5-9fe1cff30d89",
   "metadata": {},
   "source": [
    "## B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1956112c-f3a2-4c12-bc32-3f1f430894fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised:  Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//a[@class=\"si-player-name-wrap\"]/span[@class=\"si-text si-fname\"]\"}\n",
      "  (Session info: chrome=126.0.6478.63); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF71E77EEA2+31554]\n",
      "\t(No symbol) [0x00007FF71E6F7ED9]\n",
      "\t(No symbol) [0x00007FF71E5B872A]\n",
      "\t(No symbol) [0x00007FF71E608434]\n",
      "\t(No symbol) [0x00007FF71E60853C]\n",
      "\t(No symbol) [0x00007FF71E5FBBAC]\n",
      "\t(No symbol) [0x00007FF71E62D06F]\n",
      "\t(No symbol) [0x00007FF71E5FBA76]\n",
      "\t(No symbol) [0x00007FF71E62D240]\n",
      "\t(No symbol) [0x00007FF71E64C977]\n",
      "\t(No symbol) [0x00007FF71E62CDD3]\n",
      "\t(No symbol) [0x00007FF71E5FA33B]\n",
      "\t(No symbol) [0x00007FF71E5FAED1]\n",
      "\tGetHandleVerifier [0x00007FF71EA88B1D+3217341]\n",
      "\tGetHandleVerifier [0x00007FF71EAD5AE3+3532675]\n",
      "\tGetHandleVerifier [0x00007FF71EACB0E0+3489152]\n",
      "\tGetHandleVerifier [0x00007FF71E82E776+750614]\n",
      "\t(No symbol) [0x00007FF71E70375F]\n",
      "\t(No symbol) [0x00007FF71E6FEB14]\n",
      "\t(No symbol) [0x00007FF71E6FECA2]\n",
      "\t(No symbol) [0x00007FF71E6EE16F]\n",
      "\tBaseThreadInitThunk [0x00007FF9C56E257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF9C5F6AF28+40]\n",
      "\n",
      "        Name          Team Rating\n",
      "0      Babar                  824\n",
      "1    Shubman         INDIA    801\n",
      "2      Virat         INDIA    768\n",
      "3      Rohit         INDIA    746\n",
      "4      Harry       IRELAND    746\n",
      "5      Daryl   NEW ZEALAND    728\n",
      "6      David     AUSTRALIA    723\n",
      "7     Pathum     SRI LANKA    711\n",
      "8      Dawid       ENGLAND    707\n",
      "9     Rassie  SOUTH AFRICA    701\n",
      "10  Heinrich  SOUTH AFRICA    697\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def scrape_icc_batting_rankings(url):\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the ICC batting rankings page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the table to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'si-body'))\n",
    "    )\n",
    "    \n",
    "    # Lists to store the scraped data\n",
    "    player_names = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    # Locate the table rows containing the player data\n",
    "    rows = driver.find_elements(By.XPATH, '//div[@class=\"si-table-row\"]')\n",
    "    \n",
    "    # Loop through each row and extract the data\n",
    "    for row in rows[:12]:\n",
    "        try:\n",
    "            player_name = row.find_element(By.XPATH, './/a[@class=\"si-player-name-wrap\"]/span[@class=\"si-text si-fname\"]').text\n",
    "            team = row.find_element(By.XPATH, './/h3[@class=\"si-team-name\"]/span[@class=\"si-fname si-text\"]').text\n",
    "            rating = row.find_element(By.XPATH, './/div[@class=\"si-table-data si-rating\"]/span[@class=\"si-text\"]').text\n",
    "            \n",
    "            player_names.append(player_name)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Exception raised: \", e)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    # Create a DataFrame from the scraped data\n",
    "    data = {\n",
    "        'Name': player_names,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "url = 'https://www.icc-cricket.com/rankings/batting/mens/odi'\n",
    "df = scrape_icc_batting_rankings(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0033bff-794f-4fe7-9de3-778128a460c3",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc1b268-fdb6-45ce-9301-0942f5c3994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised:  Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//a[@class=\"si-player-name-wrap\"]/span[@class=\"si-text si-fname\"]\"}\n",
      "  (Session info: chrome=126.0.6478.63); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF71E77EEA2+31554]\n",
      "\t(No symbol) [0x00007FF71E6F7ED9]\n",
      "\t(No symbol) [0x00007FF71E5B872A]\n",
      "\t(No symbol) [0x00007FF71E608434]\n",
      "\t(No symbol) [0x00007FF71E60853C]\n",
      "\t(No symbol) [0x00007FF71E5FBBAC]\n",
      "\t(No symbol) [0x00007FF71E62D06F]\n",
      "\t(No symbol) [0x00007FF71E5FBA76]\n",
      "\t(No symbol) [0x00007FF71E62D240]\n",
      "\t(No symbol) [0x00007FF71E64C977]\n",
      "\t(No symbol) [0x00007FF71E62CDD3]\n",
      "\t(No symbol) [0x00007FF71E5FA33B]\n",
      "\t(No symbol) [0x00007FF71E5FAED1]\n",
      "\tGetHandleVerifier [0x00007FF71EA88B1D+3217341]\n",
      "\tGetHandleVerifier [0x00007FF71EAD5AE3+3532675]\n",
      "\tGetHandleVerifier [0x00007FF71EACB0E0+3489152]\n",
      "\tGetHandleVerifier [0x00007FF71E82E776+750614]\n",
      "\t(No symbol) [0x00007FF71E70375F]\n",
      "\t(No symbol) [0x00007FF71E6FEB14]\n",
      "\t(No symbol) [0x00007FF71E6FECA2]\n",
      "\t(No symbol) [0x00007FF71E6EE16F]\n",
      "\tBaseThreadInitThunk [0x00007FF9C56E257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF9C5F6AF28+40]\n",
      "\n",
      "        Name         Team Rating\n",
      "0     Keshav                 716\n",
      "1       Josh    AUSTRALIA    688\n",
      "2       Adam    AUSTRALIA    686\n",
      "3   Mohammed        INDIA    678\n",
      "4    Jasprit        INDIA    665\n",
      "5   Mohammad  AFGHANISTAN    656\n",
      "6    Shaheen     PAKISTAN    650\n",
      "7    Kuldeep        INDIA    645\n",
      "8      Trent  NEW ZEALAND    643\n",
      "9    Bernard      NAMIBIA    642\n",
      "10    Rashid  AFGHANISTAN    634\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def scrape_icc_bowling_rankings(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "   \n",
    "    driver.get(url)\n",
    "    \n",
    "   \n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'si-body'))\n",
    "    )\n",
    "    \n",
    "    player_names = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    rows = driver.find_elements(By.XPATH, '//div[@class=\"si-table-row\"]')\n",
    "    \n",
    "    for row in rows[:12]:\n",
    "        try:\n",
    "            player_name = row.find_element(By.XPATH, './/a[@class=\"si-player-name-wrap\"]/span[@class=\"si-text si-fname\"]').text\n",
    "            team = row.find_element(By.XPATH, './/h3[@class=\"si-team-name\"]/span[@class=\"si-fname si-text\"]').text\n",
    "            rating = row.find_element(By.XPATH, './/div[@class=\"si-table-data si-rating\"]/span[@class=\"si-text\"]').text\n",
    "            \n",
    "            player_names.append(player_name)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Exception raised: \", e)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    data = {\n",
    "        'Name': player_names,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "url = 'https://www.icc-cricket.com/rankings/bowling/mens/odi'\n",
    "df = scrape_icc_bowling_rankings(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef260d46-42c5-4a6d-a49e-251802fc9e9d",
   "metadata": {},
   "source": [
    "## QUESTION 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856a925-5d80-4f1d-87bd-ab21b6dd41ba",
   "metadata": {},
   "source": [
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "953ec1b9-82db-4e22-ba37-30251b24df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised:  Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//h3[@class=\"si-team-name\"]/span[@class=\"si-fname si-text\"]\"}\n",
      "  (Session info: chrome=126.0.6478.63); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF71E77EEA2+31554]\n",
      "\t(No symbol) [0x00007FF71E6F7ED9]\n",
      "\t(No symbol) [0x00007FF71E5B872A]\n",
      "\t(No symbol) [0x00007FF71E608434]\n",
      "\t(No symbol) [0x00007FF71E60853C]\n",
      "\t(No symbol) [0x00007FF71E5FBBAC]\n",
      "\t(No symbol) [0x00007FF71E62D06F]\n",
      "\t(No symbol) [0x00007FF71E5FBA76]\n",
      "\t(No symbol) [0x00007FF71E62D240]\n",
      "\t(No symbol) [0x00007FF71E64C977]\n",
      "\t(No symbol) [0x00007FF71E62CDD3]\n",
      "\t(No symbol) [0x00007FF71E5FA33B]\n",
      "\t(No symbol) [0x00007FF71E5FAED1]\n",
      "\tGetHandleVerifier [0x00007FF71EA88B1D+3217341]\n",
      "\tGetHandleVerifier [0x00007FF71EAD5AE3+3532675]\n",
      "\tGetHandleVerifier [0x00007FF71EACB0E0+3489152]\n",
      "\tGetHandleVerifier [0x00007FF71E82E776+750614]\n",
      "\t(No symbol) [0x00007FF71E70375F]\n",
      "\t(No symbol) [0x00007FF71E6FEB14]\n",
      "\t(No symbol) [0x00007FF71E6FECA2]\n",
      "\t(No symbol) [0x00007FF71E6EE16F]\n",
      "\tBaseThreadInitThunk [0x00007FF9C56E257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF9C5F6AF28+40]\n",
      "\n",
      "            Team Matches Points Rating\n",
      "0      AUSTRALIA      30   4889    163\n",
      "1        ENGLAND      28   3598    129\n",
      "2   SOUTH AFRICA      31   3403    110\n",
      "3          INDIA      23   2330    101\n",
      "4      SRI LANKA      14   1368     98\n",
      "5    NEW ZEALAND      27   2604     96\n",
      "6    WEST INDIES      26   2241     86\n",
      "7     BANGLADESH      20   1574     79\n",
      "8       THAILAND      11    753     68\n",
      "9       PAKISTAN      32   2072     65\n",
      "10       IRELAND      19    675     36\n",
      "11      ZIMBABWE      13    172     13\n",
      "12   NETHERLANDS       9     94     10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def scrape_icc_womens_odi_team_rankings(url):\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'si-body'))\n",
    "    )\n",
    "    \n",
    "    team_names = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "    rows = driver.find_elements(By.XPATH, '//div[@class=\"si-table-row\"]')\n",
    "    for row in rows:\n",
    "        try:\n",
    "            team_name = row.find_element(By.XPATH, './/h3[@class=\"si-team-name\"]/span[@class=\"si-fname si-text\"]').text\n",
    "            match = row.find_element(By.XPATH, './/div[@class=\"si-table-data si-matches\"]/span[@class=\"si-text\"]').text\n",
    "            point = row.find_element(By.XPATH, './/div[@class=\"si-table-data si-pts\"]/span[@class=\"si-text\"]').text\n",
    "            rating = row.find_element(By.XPATH, './/div[@class=\"si-table-data si-rating\"]/span[@class=\"si-text\"]').text\n",
    "            \n",
    "            team_names.append(team_name)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Exception raised: \", e)\n",
    "    driver.quit()\n",
    "    data = {\n",
    "        'Team': team_names,\n",
    "        'Matches': matches,\n",
    "        'Points': points,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "url = 'https://www.icc-cricket.com/rankings/team-rankings/womens/odi'\n",
    "df = scrape_icc_womens_odi_team_rankings(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a65f9-c5be-4063-b650-ae29759832af",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad9f4bb8-45b2-4172-9727-6e4ed82e1e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised:  Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//a[@class=\"si-player-name-wrap\"]/span[@class=\"si-text si-fname\"]\"}\n",
      "  (Session info: chrome=126.0.6478.63); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF71E77EEA2+31554]\n",
      "\t(No symbol) [0x00007FF71E6F7ED9]\n",
      "\t(No symbol) [0x00007FF71E5B872A]\n",
      "\t(No symbol) [0x00007FF71E608434]\n",
      "\t(No symbol) [0x00007FF71E60853C]\n",
      "\t(No symbol) [0x00007FF71E5FBBAC]\n",
      "\t(No symbol) [0x00007FF71E62D06F]\n",
      "\t(No symbol) [0x00007FF71E5FBA76]\n",
      "\t(No symbol) [0x00007FF71E62D240]\n",
      "\t(No symbol) [0x00007FF71E64C977]\n",
      "\t(No symbol) [0x00007FF71E62CDD3]\n",
      "\t(No symbol) [0x00007FF71E5FA33B]\n",
      "\t(No symbol) [0x00007FF71E5FAED1]\n",
      "\tGetHandleVerifier [0x00007FF71EA88B1D+3217341]\n",
      "\tGetHandleVerifier [0x00007FF71EAD5AE3+3532675]\n",
      "\tGetHandleVerifier [0x00007FF71EACB0E0+3489152]\n",
      "\tGetHandleVerifier [0x00007FF71E82E776+750614]\n",
      "\t(No symbol) [0x00007FF71E70375F]\n",
      "\t(No symbol) [0x00007FF71E6FEB14]\n",
      "\t(No symbol) [0x00007FF71E6FECA2]\n",
      "\t(No symbol) [0x00007FF71E6EE16F]\n",
      "\tBaseThreadInitThunk [0x00007FF9C56E257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF9C5F6AF28+40]\n",
      "\n",
      "           Name                Team Rating\n",
      "0       Natalie                        772\n",
      "1       Chamari     SRI LANKA WOMEN    768\n",
      "2        Smriti         INDIA WOMEN    715\n",
      "3          Beth     AUSTRALIA WOMEN    704\n",
      "4         Laura  SOUTH AFRICA WOMEN    702\n",
      "5        Ellyse     AUSTRALIA WOMEN    689\n",
      "6        Alyssa     AUSTRALIA WOMEN    663\n",
      "7        Hayley   WEST INDIES WOMEN    660\n",
      "8     Marizanne  SOUTH AFRICA WOMEN    654\n",
      "9        Sophie   NEW ZEALAND WOMEN    637\n",
      "10  Harmanpreet         INDIA WOMEN    625\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def scrape_icc_womens_odi_batting_rankings(url):\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    " \n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'si-body'))\n",
    "    )\n",
    "    \n",
    "   \n",
    "    player_names = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "\n",
    "    rows = driver.find_elements(By.XPATH, '//div[@class=\"si-table-row\"]')\n",
    "    \n",
    "\n",
    "    for row in rows[:12]:\n",
    "        try:\n",
    "            player_name = row.find_element(By.XPATH, './/a[@class=\"si-player-name-wrap\"]/span[@class=\"si-text si-fname\"]').text\n",
    "            team = row.find_element(By.XPATH, './/h3[@class=\"si-team-name\"]/span[@class=\"si-fname si-text\"]').text\n",
    "            rating = row.find_element(By.XPATH, './/div[@class=\"si-table-data si-rating\"]/span[@class=\"si-text\"]').text\n",
    "            \n",
    "            player_names.append(player_name)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Exception raised: \", e)\n",
    "\n",
    "    driver.quit()\n",
    " \n",
    "    data = {\n",
    "        'Name': player_names,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "url = 'https://www.icc-cricket.com/rankings/batting/womens/odi'\n",
    "df = scrape_icc_womens_odi_batting_rankings(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102a3ba-275d-4f51-bbf4-a9ac50a1b114",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41ebbb51-d2ea-4ea4-84ba-ce857bc24c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised:  Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//a[@class=\"si-player-name-wrap\"]/span[@class=\"si-text si-fname\"]\"}\n",
      "  (Session info: chrome=126.0.6478.63); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF71E77EEA2+31554]\n",
      "\t(No symbol) [0x00007FF71E6F7ED9]\n",
      "\t(No symbol) [0x00007FF71E5B872A]\n",
      "\t(No symbol) [0x00007FF71E608434]\n",
      "\t(No symbol) [0x00007FF71E60853C]\n",
      "\t(No symbol) [0x00007FF71E5FBBAC]\n",
      "\t(No symbol) [0x00007FF71E62D06F]\n",
      "\t(No symbol) [0x00007FF71E5FBA76]\n",
      "\t(No symbol) [0x00007FF71E62D240]\n",
      "\t(No symbol) [0x00007FF71E64C977]\n",
      "\t(No symbol) [0x00007FF71E62CDD3]\n",
      "\t(No symbol) [0x00007FF71E5FA33B]\n",
      "\t(No symbol) [0x00007FF71E5FAED1]\n",
      "\tGetHandleVerifier [0x00007FF71EA88B1D+3217341]\n",
      "\tGetHandleVerifier [0x00007FF71EAD5AE3+3532675]\n",
      "\tGetHandleVerifier [0x00007FF71EACB0E0+3489152]\n",
      "\tGetHandleVerifier [0x00007FF71E82E776+750614]\n",
      "\t(No symbol) [0x00007FF71E70375F]\n",
      "\t(No symbol) [0x00007FF71E6FEB14]\n",
      "\t(No symbol) [0x00007FF71E6FECA2]\n",
      "\t(No symbol) [0x00007FF71E6EE16F]\n",
      "\tBaseThreadInitThunk [0x00007FF9C56E257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF9C5F6AF28+40]\n",
      "\n",
      "         Name                Team Rating\n",
      "0   Marizanne                        425\n",
      "1      Hayley   WEST INDIES WOMEN    410\n",
      "2     Natalie       ENGLAND WOMEN    374\n",
      "3      Amelia   NEW ZEALAND WOMEN    365\n",
      "4    Ashleigh     AUSTRALIA WOMEN    364\n",
      "5      Deepti         INDIA WOMEN    356\n",
      "6      Sophie   NEW ZEALAND WOMEN    256\n",
      "7      Ellyse     AUSTRALIA WOMEN    252\n",
      "8     Chamari     SRI LANKA WOMEN    240\n",
      "9        Nida      PAKISTAN WOMEN    212\n",
      "10      Chloe  SOUTH AFRICA WOMEN    199\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def scrape_icc_womens_odi_allrounder_rankings(url):\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the ICC women's ODI all-rounder rankings page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the table to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'si-body'))\n",
    "    )\n",
    "    \n",
    "    # Lists to store the scraped data\n",
    "    player_names = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    # Locate the table rows containing the player data\n",
    "    rows = driver.find_elements(By.XPATH, '//div[@class=\"si-table-row\"]')\n",
    "    \n",
    "    # Loop through each row and extract the data\n",
    "    for row in rows[:12]:\n",
    "        try:\n",
    "            player_name = row.find_element(By.XPATH, './/a[@class=\"si-player-name-wrap\"]/span[@class=\"si-text si-fname\"]').text\n",
    "            team = row.find_element(By.XPATH, './/h3[@class=\"si-team-name\"]/span[@class=\"si-fname si-text\"]').text\n",
    "            rating = row.find_element(By.XPATH, './/div[@class=\"si-table-data si-rating\"]/span[@class=\"si-text\"]').text\n",
    "            \n",
    "            player_names.append(player_name)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Exception raised: \", e)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    # Create a DataFrame from the scraped data\n",
    "    data = {\n",
    "        'Name': player_names,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "url = 'https://www.icc-cricket.com/rankings/allrounder/womens/odi'\n",
    "df = scrape_icc_womens_odi_allrounder_rankings(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69afc8b3-d060-4a3f-8b78-c603601e7eb5",
   "metadata": {},
   "source": [
    "## QUESTION 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b767100e-6e58-4161-93b6-82a121f1a5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Writer                                              Title  \\\n",
      "0          Kif Leswing  Nvidia slides 13% in three days after briefly ...   \n",
      "1          Kif Leswing  These ETFs have beaten the S&P 500 for five ye...   \n",
      "2           Ganesh Rao  Oracle warns that a TikTok ban would hurt busi...   \n",
      "3           Ganesh Rao  U.S. probing China Telecom, China Mobile over ...   \n",
      "4         Jordan Novet  Stock futures are little changed after Nasdaq ...   \n",
      "5         Jordan Novet  CNBC Daily Open: Dow jumps as investors rotate...   \n",
      "6       Samantha Subin  This stock is 'one of the cheapest ways' to pl...   \n",
      "7       Samantha Subin  34-year-old making $400,000 at TikTok: Being l...   \n",
      "8             Abid Ali  China employing 'gray zone tactics' at contest...   \n",
      "9             Abid Ali  Ferrari CEO says all-electric model preserves ...   \n",
      "10         Weizhen Tan  Julian Assange has reached a plea deal with th...   \n",
      "11         Weizhen Tan  Fast fashion retailer Shein confidentially fil...   \n",
      "12        Robert Frank  Dow closes more than 250 points higher as inve...   \n",
      "13        Robert Frank  China wants EU to scrap planned higher tariffs...   \n",
      "14  Gabrielle Fonrouge  Southeast Asia is a top choice for firms diver...   \n",
      "15  Gabrielle Fonrouge  Singaporean swimmer Toh Wei Soong hopes for a ...   \n",
      "16  Hakyung Kim,Yun Li    Bitcoin tumbles below $60,000 to start the week   \n",
      "17  Hakyung Kim,Yun Li  South Korea plant fire kills 22 people after l...   \n",
      "18       Sheila Chiang  OpenAI reverses controversial stock sale polic...   \n",
      "19       Sheila Chiang  Princess Anne sustains minor injuries and a co...   \n",
      "20       Sheila Chiang                                               None   \n",
      "21       Sheila Chiang                                               None   \n",
      "22          Vivien Soo                                               None   \n",
      "23          Vivien Soo                                               None   \n",
      "24      Tanaya Macheel                                               None   \n",
      "25      Tanaya Macheel                                               None   \n",
      "26   Ruxandra Iordache                                               None   \n",
      "27   Ruxandra Iordache                                               None   \n",
      "28        Hayden Field                                               None   \n",
      "29        Hayden Field                                               None   \n",
      "\n",
      "           Time  \n",
      "0    41 min ago  \n",
      "1    41 min ago  \n",
      "2   5 hours ago  \n",
      "3   5 hours ago  \n",
      "4          None  \n",
      "5          None  \n",
      "6          None  \n",
      "7          None  \n",
      "8          None  \n",
      "9          None  \n",
      "10         None  \n",
      "11         None  \n",
      "12         None  \n",
      "13         None  \n",
      "14         None  \n",
      "15         None  \n",
      "16         None  \n",
      "17         None  \n",
      "18         None  \n",
      "19         None  \n",
      "20         None  \n",
      "21         None  \n",
      "22         None  \n",
      "23         None  \n",
      "24         None  \n",
      "25         None  \n",
      "26         None  \n",
      "27         None  \n",
      "28         None  \n",
      "29         None  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Writer</th>\n",
       "      <th>Title</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kif Leswing</td>\n",
       "      <td>Nvidia slides 13% in three days after briefly ...</td>\n",
       "      <td>41 min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kif Leswing</td>\n",
       "      <td>These ETFs have beaten the S&amp;P 500 for five ye...</td>\n",
       "      <td>41 min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ganesh Rao</td>\n",
       "      <td>Oracle warns that a TikTok ban would hurt busi...</td>\n",
       "      <td>5 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ganesh Rao</td>\n",
       "      <td>U.S. probing China Telecom, China Mobile over ...</td>\n",
       "      <td>5 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jordan Novet</td>\n",
       "      <td>Stock futures are little changed after Nasdaq ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jordan Novet</td>\n",
       "      <td>CNBC Daily Open: Dow jumps as investors rotate...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Samantha Subin</td>\n",
       "      <td>This stock is 'one of the cheapest ways' to pl...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Samantha Subin</td>\n",
       "      <td>34-year-old making $400,000 at TikTok: Being l...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Abid Ali</td>\n",
       "      <td>China employing 'gray zone tactics' at contest...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Abid Ali</td>\n",
       "      <td>Ferrari CEO says all-electric model preserves ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Weizhen Tan</td>\n",
       "      <td>Julian Assange has reached a plea deal with th...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Weizhen Tan</td>\n",
       "      <td>Fast fashion retailer Shein confidentially fil...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Robert Frank</td>\n",
       "      <td>Dow closes more than 250 points higher as inve...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Robert Frank</td>\n",
       "      <td>China wants EU to scrap planned higher tariffs...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gabrielle Fonrouge</td>\n",
       "      <td>Southeast Asia is a top choice for firms diver...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Gabrielle Fonrouge</td>\n",
       "      <td>Singaporean swimmer Toh Wei Soong hopes for a ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hakyung Kim,Yun Li</td>\n",
       "      <td>Bitcoin tumbles below $60,000 to start the week</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hakyung Kim,Yun Li</td>\n",
       "      <td>South Korea plant fire kills 22 people after l...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sheila Chiang</td>\n",
       "      <td>OpenAI reverses controversial stock sale polic...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sheila Chiang</td>\n",
       "      <td>Princess Anne sustains minor injuries and a co...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sheila Chiang</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sheila Chiang</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Vivien Soo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Vivien Soo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Tanaya Macheel</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Tanaya Macheel</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ruxandra Iordache</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ruxandra Iordache</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hayden Field</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Hayden Field</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Writer                                              Title  \\\n",
       "0          Kif Leswing  Nvidia slides 13% in three days after briefly ...   \n",
       "1          Kif Leswing  These ETFs have beaten the S&P 500 for five ye...   \n",
       "2           Ganesh Rao  Oracle warns that a TikTok ban would hurt busi...   \n",
       "3           Ganesh Rao  U.S. probing China Telecom, China Mobile over ...   \n",
       "4         Jordan Novet  Stock futures are little changed after Nasdaq ...   \n",
       "5         Jordan Novet  CNBC Daily Open: Dow jumps as investors rotate...   \n",
       "6       Samantha Subin  This stock is 'one of the cheapest ways' to pl...   \n",
       "7       Samantha Subin  34-year-old making $400,000 at TikTok: Being l...   \n",
       "8             Abid Ali  China employing 'gray zone tactics' at contest...   \n",
       "9             Abid Ali  Ferrari CEO says all-electric model preserves ...   \n",
       "10         Weizhen Tan  Julian Assange has reached a plea deal with th...   \n",
       "11         Weizhen Tan  Fast fashion retailer Shein confidentially fil...   \n",
       "12        Robert Frank  Dow closes more than 250 points higher as inve...   \n",
       "13        Robert Frank  China wants EU to scrap planned higher tariffs...   \n",
       "14  Gabrielle Fonrouge  Southeast Asia is a top choice for firms diver...   \n",
       "15  Gabrielle Fonrouge  Singaporean swimmer Toh Wei Soong hopes for a ...   \n",
       "16  Hakyung Kim,Yun Li    Bitcoin tumbles below $60,000 to start the week   \n",
       "17  Hakyung Kim,Yun Li  South Korea plant fire kills 22 people after l...   \n",
       "18       Sheila Chiang  OpenAI reverses controversial stock sale polic...   \n",
       "19       Sheila Chiang  Princess Anne sustains minor injuries and a co...   \n",
       "20       Sheila Chiang                                               None   \n",
       "21       Sheila Chiang                                               None   \n",
       "22          Vivien Soo                                               None   \n",
       "23          Vivien Soo                                               None   \n",
       "24      Tanaya Macheel                                               None   \n",
       "25      Tanaya Macheel                                               None   \n",
       "26   Ruxandra Iordache                                               None   \n",
       "27   Ruxandra Iordache                                               None   \n",
       "28        Hayden Field                                               None   \n",
       "29        Hayden Field                                               None   \n",
       "\n",
       "           Time  \n",
       "0    41 min ago  \n",
       "1    41 min ago  \n",
       "2   5 hours ago  \n",
       "3   5 hours ago  \n",
       "4          None  \n",
       "5          None  \n",
       "6          None  \n",
       "7          None  \n",
       "8          None  \n",
       "9          None  \n",
       "10         None  \n",
       "11         None  \n",
       "12         None  \n",
       "13         None  \n",
       "14         None  \n",
       "15         None  \n",
       "16         None  \n",
       "17         None  \n",
       "18         None  \n",
       "19         None  \n",
       "20         None  \n",
       "21         None  \n",
       "22         None  \n",
       "23         None  \n",
       "24         None  \n",
       "25         None  \n",
       "26         None  \n",
       "27         None  \n",
       "28         None  \n",
       "29         None  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_world_news(url):\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    writers = []\n",
    "    for writer in soup.find_all('span', class_='RiverByline-authorByline'):\n",
    "        writers.append(writer.text.strip())\n",
    "\n",
    "    titles = []\n",
    "    for title in soup.find_all('div', class_='RiverHeadline-headline RiverHeadline-hasThumbnail'):\n",
    "        titles.append(title.text.strip())\n",
    "\n",
    "    times = []\n",
    "    for time in soup.find_all('span', class_='RiverByline-datePublished'):\n",
    "        times.append(time.text.strip())\n",
    "\n",
    "    max_len = max(len(writers), len(titles), len(times))\n",
    "    writers.extend([None] * (max_len - len(writers)))\n",
    "    titles.extend([None] * (max_len - len(titles)))\n",
    "    times.extend([None] * (max_len - len(times)))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Writer': writers,\n",
    "        'Title': titles,\n",
    "        'Time': times\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "df = scrape_cnbc_world_news(url)\n",
    "print(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a911ce7-c418-4b7c-a4b5-ecdfd81acf04",
   "metadata": {},
   "source": [
    "## QUESTION 8 - LINK INVALID OR RIDIRECTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e480e41-c0b1-45a8-871a-52ff852f39cc",
   "metadata": {},
   "source": [
    "## QUESTION 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83b98968-6642-4020-9899-a28489cdd52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name  \\\n",
      "0                 Castle's Barbeque   \n",
      "1                        Cafe Knosh   \n",
      "2                       India Grill   \n",
      "3              The Barbeque Company   \n",
      "4                    Delhi Barbeque   \n",
      "5  The Monarch - Bar Be Que Village   \n",
      "\n",
      "                                            Location Rating  \\\n",
      "0                     Connaught Place, Central Delhi      4   \n",
      "1  The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
      "2               Hilton Garden Inn,Saket, South Delhi    3.9   \n",
      "3                 Gardens Galleria,Sector 38A, Noida    3.9   \n",
      "4     Taurus Sarovar Portico,Mahipalpur, South Delhi    3.7   \n",
      "5  Indirapuram Habitat Centre,Indirapuram, Ghaziabad    3.8   \n",
      "\n",
      "                                               Image       Cuisine  \n",
      "0  https://im1.dineout.co.in/images/uploads/resta...       Chinese  \n",
      "1  https://im1.dineout.co.in/images/uploads/resta...  North Indian  \n",
      "2  https://im1.dineout.co.in/images/uploads/resta...       Italian  \n",
      "3  https://im1.dineout.co.in/images/uploads/resta...   Continental  \n",
      "4  https://im1.dineout.co.in/images/uploads/resta...  North Indian  \n",
      "5  https://im1.dineout.co.in/images/uploads/resta...       Italian  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Image</th>\n",
       "      <th>Cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle's Barbeque</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>North Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Continental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>North Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant Name  \\\n",
       "0                 Castle's Barbeque   \n",
       "1                        Cafe Knosh   \n",
       "2                       India Grill   \n",
       "3              The Barbeque Company   \n",
       "4                    Delhi Barbeque   \n",
       "5  The Monarch - Bar Be Que Village   \n",
       "\n",
       "                                            Location Rating  \\\n",
       "0                     Connaught Place, Central Delhi      4   \n",
       "1  The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
       "2               Hilton Garden Inn,Saket, South Delhi    3.9   \n",
       "3                 Gardens Galleria,Sector 38A, Noida    3.9   \n",
       "4     Taurus Sarovar Portico,Mahipalpur, South Delhi    3.7   \n",
       "5  Indirapuram Habitat Centre,Indirapuram, Ghaziabad    3.8   \n",
       "\n",
       "                                               Image       Cuisine  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...       Chinese  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  North Indian  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...       Italian  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...   Continental  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  North Indian  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...       Italian  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_buffet_specials(url):\n",
    "    # Fetch the webpage content\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    restaurant_names = [restaurant.text for restaurant in soup.find_all('a', class_='restnt-name ellipsis')]\n",
    "    locations = [location.text for location in soup.find_all('div', class_='restnt-loc ellipsis')]\n",
    "    ratings = [rating.text for rating in soup.find_all('div', class_='restnt-rating rating-4')]\n",
    "    images = [image['data-src'] for image in soup.find_all('img', class_='no-img')]\n",
    "    cuisines = [cuisine.text for cuisine in soup.find_all('a', attrs={'data-w-onclick': 'stopClickPropagation|w1-restarant'})]\n",
    "\n",
    "    max_len = max(len(restaurant_names), len(locations), len(ratings), len(images), len(cuisines))\n",
    "    restaurant_names.extend([None] * (max_len - len(restaurant_names)))\n",
    "    locations.extend([None] * (max_len - len(locations)))\n",
    "    ratings.extend([None] * (max_len - len(ratings)))\n",
    "    images.extend([None] * (max_len - len(images)))\n",
    "    cuisines.extend([None] * (max_len - len(cuisines)))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Restaurant Name': restaurant_names,\n",
    "        'Location': locations,\n",
    "        'Rating': ratings,\n",
    "        'Image': images,\n",
    "        'Cuisine': cuisines\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "df = scrape_dineout_buffet_specials(url)\n",
    "print(df.head(6))\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4131db3-721d-49c1-8c0b-e4ebadea35c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
